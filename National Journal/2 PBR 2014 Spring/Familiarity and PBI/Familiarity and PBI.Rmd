Exploring Familiarity and Policy Brand Perception
===
####A short analysis examining the relationship between an organization's familiarity and it's policy brand perception among policy influentials

*by: D'Cypher*

Are the most well known organizations the most influential brands in Washington? That is the question we typically recieve from our partners. Our partners often characterize themselves as the "little guy" and believe that only the organizations with big brand names, enormous budgets, and an abundance of lobbyists and powerful connections can be effective inside the beltway. Our partners are often left wondering how they can compete in such an environment. However, our research shows that there is an oppertunity, and it might not be the case that the well know organizations are the most successful in Washington.

The policy community is vast; even if an government affairs office can "buy" the influence of each member of this community for a mere $100, not even the biggest budget in the world could buy all of them. Money is only a small part of the equation in affecting policy. Money might be highly effective in an ad hoc basis, but it does not guarantee any long term impact. It is more important for an organization to have a profile that is distinguished, highly thought of, and well respected among the policy making community. That is they key to establishing a lasting presence in the matters of national public policy. 

###Synopsis

In our research, we found that the big brand names, who are very familiar among policy influentials, are not necessarily the highest performing ones; rather, it is more important that an organization focus on the quality of engagement with the individuals that they work with. However, that is not to say familiarity does not matter, as in certain cases organizations maybe struggling their communication, message, or ability to gain visibility.

The Policy Brands Roundtable (PBR) study is National Journal Research's strategic research initiative designed to understand the perceptions of Washington influentials. PBR seeks to understand their views on the major organizations, industries, and players relevant their policy expertise area. Over the course of two years of this study (Fall 2013 to Spring 2015), we've surveyed over 6,000 policy influentials and studied nearly 200 organizations across more than 20 industries. 

In the research design, we created a model that measures the policy community's perception of an organization's policy work, reflected what we call the Policy Brand Index score (PBI). Similar to consumer brand, PBI is designed to capture the perception and the overall policy profile of an organization. Further, we also measured each respondent's familiarity with the organization's policy work, measured on a 1 to 5 scale. This reflects how well known the organization is within the policy community. 

This article will show how familiarity does not in itself translate to a strong policy brand performance, but rather the building strong relationships and having quality engagements with policy makers is much more important.

###Data Processing
The analyses preform in this project were primarily executed in R. In order to replicate the results described in this overview, please first make sure the following codes run correctly in your R environment.

Important note: Please make sure to change all "setwd()" to your working directory that contain the required data files.

```{r results='hide', message=FALSE, warning=FALSE}
#Loads the relevant data, combined data from Fall 2013 to Spring 2015.
#NOTE: Make sure to change setwd() to your directory that contains the relevant data files.
setwd("C:/Users/wcai/Desktop/5 Personal Projects/xSharable BLIND Data/PBR Coh 4 Data BLIND sharable/")
AllCoh_resplv <- read.csv("Coh1to4 Combined 8_12_15 BLIND.csv")
AllCoh_orglv <- read.csv("Coh 4 Comp lv Data 8_12_15 BLIND.csv")
require(plyr)
require(dplyr)
require(ggplot2)
require(gridExtra)
require(scales)
require(reshape2)
require(devtools)
require(proto)

Latestcoh_Orgs <- AllCoh_orglv %>% subset(Latest.Org.Data.CL == 1)
Latestcoh_Corps <- AllCoh_orglv %>% subset(Asso.CL == 0 & Latest.Org.Data.CL == 1)
Latestcoh_Asso <- AllCoh_orglv %>% subset(Asso.CL == 1 & Latest.Org.Data.CL == 1)
c4only_Orgs <- AllCoh_orglv %>% subset(Coh.CL == 4 & Latest.Org.Data.CL == 1)
c4only_Corps <- AllCoh_orglv %>% subset(Coh.CL == 4 & Asso.CL == 0 & Latest.Org.Data.CL == 1)
c4only_Asso <- AllCoh_orglv %>% subset(Coh.CL == 4 & Asso.CL == 1 & Latest.Org.Data.CL == 1)

#R-squared value function
lm_r2 = function(df, y, x){
  m = summary(lm(y ~ x, df))$r.squared;
  r2 <- paste("r^2 = ", format(m, digits = 3));
  as.character(as.expression(r2));                 
}
```

The following code preforms several transformation and calculation for the analyses in this overview:

```{r message=FALSE, warning=FALSE}
###====Create subset of composite scores and Ns by 3s 4s 5s
AllCoh_resplv_M <- AllCoh_resplv %>%
  select(Company.Unique.Code.CYPHER,Composite,Familiarity.Completes.Only)

AllCoh_resplv_M <- AllCoh_resplv_M %>%
  mutate(Fam3Composite = ifelse(Familiarity.Completes.Only == 3, 
                                Composite,NA)) %>%
  mutate(Fam4Composite = ifelse(Familiarity.Completes.Only == 4, 
                                Composite,NA)) %>%
  mutate(Fam5Composite = ifelse(Familiarity.Completes.Only == 5, 
                                Composite,NA)) %>%
  mutate(Fam5or4Composite = ifelse(Familiarity.Completes.Only > 3, 
                                   Composite,NA))

#Averageifs and Countifs
Output_DT<- AllCoh_resplv_M %>%
  select(Company.Unique.Code.CYPHER,Composite,Fam3Composite,Fam4Composite,Fam5Composite,Fam5or4Composite) %>%
  group_by(Company.Unique.Code.CYPHER) %>% 
  summarise_each(funs(Avg=mean(., na.rm=TRUE), 
                      Ns=sum(!is.na(.))))

cohL_Orgs_FamCut <- Latestcoh_Orgs %>%
  select(Company.Unique.Code.CYPHER.CL,Asso.CL,Main.Comparison.CL,Percent.Familiar.CL)

#Index Match (Merge(Select)) - Company Unique Code
cohL_Orgs_FamCut <- merge(cohL_Orgs_FamCut, Output_DT, by.x = c("Company.Unique.Code.CYPHER.CL"), 
                          by.y = c("Company.Unique.Code.CYPHER"), all.x = TRUE)
cohL_Orgs_FamCut$Composite_Avg = cohL_Orgs_FamCut$Composite_Avg * 10
cohL_Orgs_FamCut$Fam3Composite_Avg = cohL_Orgs_FamCut$Fam3Composite_Avg * 10
cohL_Orgs_FamCut$Fam4Composite_Avg = cohL_Orgs_FamCut$Fam4Composite_Avg * 10
cohL_Orgs_FamCut$Fam5Composite_Avg = cohL_Orgs_FamCut$Fam5Composite_Avg * 10
cohL_Orgs_FamCut$Fam5or4Composite_Avg = cohL_Orgs_FamCut$Fam5or4Composite_Avg * 10

cohL_Orgs_FamCut <- cohL_Orgs_FamCut %>%
  mutate(Diff.5s.and.3s = Fam5Composite_Avg - Fam3Composite_Avg) %>%
  mutate(Diff.5s.and.4s = Fam5Composite_Avg - Fam4Composite_Avg) %>%
  mutate(Diff.4s.and.3s = Fam4Composite_Avg - Fam3Composite_Avg) %>%
  mutate(Diff.5or4.and.3s = Fam5or4Composite_Avg - Fam3Composite_Avg) %>%
  mutate(Diff.5or4.and.Composite = Fam5or4Composite_Avg - Composite_Avg) %>%
  mutate(Diff.3s.and.Composite = Fam3Composite_Avg - Composite_Avg) %>%
  mutate(AvgAllOrgs = mean(cohL_Orgs_FamCut$Composite_Avg)) %>%
  mutate(Diff.Composite.and.AvgAllOrgs = Composite_Avg - AvgAllOrgs) %>%
  mutate(PercntDiff.5or4.and.3s = Diff.5or4.and.3s / (Diff.5or4.and.3s + Composite_Avg)) %>%
  mutate(PercntDiff.Composite.and.AvgAllOrgs = Diff.Composite.and.AvgAllOrgs / (Diff.Composite.and.AvgAllOrgs + Composite_Avg))


#Cleanup
rm(Output_DT);rm(AllCoh_resplv_M)
```

Brief note on the data: The two data sheets above contain data the entire data set for all four installments of the Policy Brands Roundtable study (as of Spring 2015). The first data set contains the individual evaluations of each organization. The second data set contains the organization's aggregate scores on each of the measures, which is calculated by averaging across the individual evaluations for each organization. The first data set is meant for analyzing respondent trends a information, whereas the second data set is meant for understanding organizational trends and relationships. 

While most of the findings in this overview were discovered in the Spring of 2014, they have been confirmed and updated with the latest data (as of Spring 2015).

###Research Design

In our study, we sampled senior level policy makers, specifically those that have worked on policy relevant to the organizations in our study. Only survey respondents that were familiar enough with a particular organization's policy work in Washington could evaluate that organization. Respondent had to have rated the organization a '3' or greater (scale of 1 to 5) on familiarity to be considered elgible to evaluate that organization. Respondents that indicated less than '3' were considered inelgible and were not allowed to evaluate. 

If a respondent is elgible, he would be allowed to rate the organization on 16 performance metrics (called "drivers") and 4 perception measures (called "indices"). These 4 perception measures are:
 * Respect - respect for the organization's role in the public debate
 * Consideration - genuine consideration of the organization's positions on issues
 * Influence - organization's views influencing policy maker's views favorably
 * Sharing - the willingness of the policy maker to seek out or share the organization's opinion on issues

The Policy Brand Index score (PBI) is a composite of these four perception measures. This overview will primarily focus on PBI score and the familiarity rating. For more detailed explaination of the PBI model and how it was developed, please refer to the articles "What is Policy Brand (Part 1)" and "(Part 2)". 

With regard to survey administration, we did our best to make sure that each of the organizations in the study had atleast 100 evaluations from different policy influentials, so that there are no issues with low responses or unreliable Policy Brand Scores for any particular organization.

The following is a histogram showing the distribution of Ns across all of the organizations studied:

```{r message=FALSE, warning=FALSE}
###====Histogram of Ns per Organization 
ggplot(data = Latestcoh_Orgs, aes(x=Responses.CL)) +
  geom_histogram(col = "springgreen4", fill = "seagreen3", alpha = .5, size=0, binwidth=10)+
  xlab("Number of Evaluations Per Organization (Ns)") + 
  ylab("Number of Organizations") + 
  theme(legend.position="none") +
  scale_y_continuous(limits=c(0, 30)) +
  ggtitle("Nearly Every Organization Has\nClose to 100 Evaluations or More")
```

###Policy Brand Perception and Number of People Familiar with the Brand

As discussed in the previous section, we are able to gauge an organization's general familiarity among respondents by looking at the percentage of our sample respondents that were elgible to rate the organization. We called this statistic: "Precentage of Total Respondents Familiar with the Organization" or "Percent Familiar" for short. 

To illistrate: if an organization has a 60% Percent Familiar, that means of all the respondents that were asked (on a 1 to 5 scale) how familiar they were with that organization's policy work, 60% gave it a '3' or better. 

The following is a plot of the relationship (or lack thereof) between the Percent Familiar and the Policy Brand Index score:

```{r message=FALSE, warning=FALSE}
###====All Coh: Corps and Asso PBI vs Familiarity
#Latest data on all Unique Organizations in All four studies
qplot(Latestcoh_Orgs$Percent.Familiar.CL,Latestcoh_Orgs$Composite.CL, 
           xlab = "% of Total Respondents Familiar with the Org.", 
           ylab = "Policy Brand Index Score") + 
  scale_x_continuous(limits=c(.05, .65), labels = percent) + 
  scale_y_continuous(limits=c(35, 65)) + 
  geom_smooth(method = "lm", se=FALSE, aes(group=1)) +
  geom_point(size = 3, alpha = .3) +
  annotate("text", label = lm_r2(Latestcoh_Orgs,Latestcoh_Orgs$Composite.CL,
                                 Latestcoh_Orgs$Percent.Familiar.CL), x = .6, y = 53, size = 2.5) +
  ggtitle("No Relationship b/w PBI and Organization Familiarity\n(All Organizations, All Four Studies)")
```

The following is the same plot, but isolating only organizations from the our latest study (Spring 2015), and splitting out Assocations and Corportations:

```{r message=FALSE, warning=FALSE}
###====Coh4: Corps and Asso PBI vs Familiarity
#All Coh Corp Scatter
allcorp_plot = qplot(c4only_Corps$Percent.Familiar.CL,c4only_Corps$Composite.CL, 
           xlab = "Percent Familiar", 
           ylab = NULL) + 
  scale_x_continuous(limits=c(.05, .65), labels = percent) + 
  scale_y_continuous(limits=c(35, 65), labels = NULL) + 
  geom_smooth(method = "lm", se=FALSE, aes(group=1)) +
  geom_point(size = 3, alpha = .3) +
  annotate("text", label = lm_r2(c4only_Corps,c4only_Corps$Composite.CL,
                                 c4only_Corps$Percent.Familiar.CL), x = .55, y = 51.5, size = 2.5) +
  ggtitle("Corporations Only (Spring 2015)\n")

#All Coh Asso Scatter
allasso_plot = qplot(c4only_Asso$Percent.Familiar.CL,c4only_Asso$Composite.CL, 
           xlab = "Percent Familiar", 
           ylab = "Policy Brand Index Score") + 
  scale_x_continuous(limits=c(.05, .65), labels = percent) + 
  scale_y_continuous(limits=c(35, 65)) + 
  geom_smooth(method = "lm", se=FALSE, aes(group=1)) +
  geom_point(size = 3, alpha = .3) +
  annotate("text", label = lm_r2(c4only_Asso,c4only_Asso$Composite.CL,
                                 c4only_Asso$Percent.Familiar.CL), x = .58, y = 55.5, size = 2.5) +
  ggtitle("Associations Only (Spring 2015)\n")

grid.arrange(allasso_plot, allcorp_plot, ncol=2)

```

From these plots, there does not appear to be any direct relationship between organizational familiarity and Policy Brand. An organization that is recongnized by many respondents can preform just as well or just as poorly as an organization that is relatively unknown. 

As shown in the Research Design section earlier, the majority of our sampled organization had around 100 to 120 evaluations each. Hence, the findings are not a result of an unreliable number of the evaluations. Further, there does not seem to be any time effect or organization type (whether corporation or assocation) effect that is confounding the finding. Although, associations seem to have a slightly stronger effect, the r-squared values are so small that it would be a strech to suggest a relationship.

There is an inharent research design decision that could pose a problem to this particular analysis. It has to do with the assumptions of PBI, specifically the fact that there is an elgibility requirement for evaluating an organization's PBI. Only respondents familiar enough with the organization's policy work are allowed to evaluate that organization. Hence, the views of individuals with below typical familiarity are not captured in the index. 

The fear is that a relationship may exist if our PBI score had been designed differently, specifically by allowing individuals that lack familiarity to evaluate the organization. While there isn't a conterfactual to prove or disprove this criticism, further analysis will show that this senario is unlikely. More on this later.

###The Simpson's Paradox Between Familiarity and PBI

While overall organization familiarity does not appear to have any relationship to PBI, the story is not so simple after splitting the data based the spectrum of familiarity. 

The following is a boxplot of organizations' PBI score at each level of familiarity:

```{r message=FALSE, warning=FALSE}
###====Organizations' PBI Score at Each Level of Familiarity
cohL_Orgs_FamCut_MELT <- cohL_Orgs_FamCut %>%
  select(Company.Unique.Code.CYPHER.CL, Asso.CL, Main.Comparison.CL, 
         Fam3Composite_Avg, Fam4Composite_Avg, Fam5Composite_Avg, 
         Fam3Composite_Ns, Fam4Composite_Ns, Fam5Composite_Ns) %>%
  #melt(id=c("Company.Unique.Code.CYPHER.CL", "Asso.CL", "Main.Comparison.CL"), value.name = "Composite.score") %>%
  reshape(varying = list(c("Fam3Composite_Avg", "Fam4Composite_Avg", "Fam5Composite_Avg"), 
                         c("Fam3Composite_Ns", "Fam4Composite_Ns", "Fam5Composite_Ns")), 
          v.names=c("Composite.score","Fam.Ns"),
          times = c("Org. PBI for Respondents That \n Gave '3' Ratings on Fam.",
                    "Org. PBI for Respondents That \n Gave '4' Ratings on Fam.",
                    "Org. PBI for Respondents That \n Gave '5' Ratings on Fam."),
          sep = "_", direction = "long") %>%
  mutate(Fam_score = ifelse(time == "Org. PBI for Respondents That \n Gave '3' Ratings on Fam.","3",
                            ifelse(time == "Org. PBI for Respondents That \n Gave '4' Ratings on Fam.","4",
                                   ifelse(time == "Org. PBI for Respondents That \n Gave '5' Ratings on Fam.","5",NA)))) %>%
  mutate(Facet_Labels = ifelse(Asso.CL == 0, "Corporations", 
                               ifelse(Asso.CL == 1, "Associations", NA)))

#Boxplots of the difference bw 5s,4s,3s
ggplot(data = cohL_Orgs_FamCut_MELT, aes(x=Fam_score, y=Composite.score))+
  xlab("Familiarity Rating") + 
  ylab("Policy Brand Index Score") + 
  geom_jitter(aes(x=Fam_score, y=Composite.score),
              position=position_jitter(width=0.1,height=0),
              alpha=0.6,
              size=1.5,
              show_guide=FALSE) +
  geom_boxplot(aes(fill=Fam_score, color=Fam_score), alpha=I(0.5), width=0.4) +
  theme(legend.position="none") +
  facet_grid(. ~ Facet_Labels) +
  ggtitle("Respondents that Know the\nOrganization Better, Evaluates it Higher")
```

After we split the PBI scores for each organization by the respondents *based on their familiarity,* there is a clear trend that the higher familiarity respondents evaluated the organization higher. 

The series of box-plots describe the range of scores given by respondents that indicated either a '3', '4', or '5' on familiarity for each of the organizations studied. In general, the higher the respondent's familiarity rating the higher they evaluated organizations. In addition, the differences among these groups are all statistically significant. Hence, the insight seems to be that the more familiar a respondent is with an organization's work the higher he tends to rate that organization.

The spread of scores also seems to increase. This suggest that as familiarity increases respondents also breed more extreme views toward an organization. Although they tend tp be more positive than lesser familiar respondents.

These findings seem contridictory to the previous analysis that suggests a lack of relationship between how well know an organization is and it's performance. Intuitively, there appears to be a contridiction here because more well known organizations should have more familiar respondents represented. Since those respondents tend to rate higher, it logically should follow that well know organizations would score higher. However, the data does not reflect such logic.

This is an emperical case of the effect known as the Simpson's Paradox. This statistical paradox describes a phenomanon in which trends appear when the data is grouped a certain way, but disappears when aggrigated or grouped a different way. Ofter times there is a lurking variable causing of the apparent contridiction.

Just to show that having a higher number of more familiar respondents does not translate to higher PBI, These are three charts of the number of '3s', '4s', and '5s', who gave PBI evaluations, plotted against PBI scores at each level of familiarity: 

```{r message=FALSE, warning=FALSE}
#Plot number of 5s, 4s, 3s vs PBI scores for 5s, 4s, 3s
ggplot(data = cohL_Orgs_FamCut_MELT, aes(x=Fam.Ns, y=Composite.score))+
  xlab("Number of Familiarity Rating From Respondents\nWho Also Rated that Organization on PBI") + 
  ylab("Policy Brand Index Score") + 
  geom_point(size = 1.5, alpha = .6) + 
  geom_smooth(method = "lm", se=TRUE, aes(group=1)) +
  facet_grid(. ~ time) +
  ggtitle("No Relationship b/w Number of \n High Familiarity Ratings and Higher Scores")
```

The three pannels above describe the number of ratings each organization recieved against it's PBI score at different familiarity levels. For example, the point on the far right of the far right pannel represents an organization that had just shy of 80 respondents that gave it '5' on familiarity. Those 80 respondents graded the organization just above 60 out of 100 on PBI. 

By splitting out PBI scores and number of familiarity ratings in this way bypasses any potential confounding issues that might exist mentioned in the previous section (the issue regarding elgibility requirements for evaluating PBI). Still, the finding holds, having more very familiar respondents does not equate to higher PBI scores. 

There is another observation noted in comparing these three pannels: the difference in dispursion among the three plots. There is a clear difference in the range of the PBI distribution as we move from '3s' to '4s' to '5s'. The more familiar the respondents are, the greater the increase in spread among PBI evaluations. 

Even though in the aggregate, the regression shows a lack of relationship, the difference in the spread of scores becomes evident when the data is split. This reveals a central clue to the lurking variable, which seems to have to do with the magnitude of the disparity between respondents. At the high levels of familiarity respondents seem to have more extreme views, whereas at the typical level of familiarity, respondents evaluations are much less spread. 

###The Familiarity Premium

The following is a plot that shows PBI scores compared to the difference in scores between the most familiar and typical familiar respondents:

```{r message=FALSE, warning=FALSE}
#What's really driving the difference b/w 3s,4s,5s 'High Familiarity Premium'
fam3s_plot <- ggplot(data = cohL_Orgs_FamCut, aes(x=Fam3Composite_Avg, y=Diff.5s.and.3s))+
  xlab("Org. PBI for '3' Ratings on Familiarity") + 
  #scale_x_continuous(labels = NULL) +
  ylab("Diff. b/w 5s and 3s") +
  geom_point(size = 1.5, alpha = .6) + 
  geom_smooth(method = "lm", se=FALSE, aes(group=1)) +
  annotate("text", label = lm_r2(cohL_Orgs_FamCut,cohL_Orgs_FamCut$Diff.5s.and.3s,
                                 cohL_Orgs_FamCut$Fam3Composite_Avg), x = 38.5, y = 12.6, size = 2.5) +
  ggtitle("PBI Scores of 5's Seem to be Driving the Difference")

fam5s_plot <- ggplot(data = cohL_Orgs_FamCut, aes(x=Fam5Composite_Avg, y=Diff.5s.and.3s))+
  xlab("Org. PBI for '5' Ratings on Familiarity") + 
  ylab("Diff. b/w 5s and 3s") + 
  #scale_y_continuous(labels = NULL) +
  geom_point(size = 1.5, alpha = .6) + 
  geom_smooth(method = "lm", se=FALSE, aes(group=1)) +
  annotate("text", label = lm_r2(cohL_Orgs_FamCut,cohL_Orgs_FamCut$Diff.5s.and.3s,
                                 cohL_Orgs_FamCut$Fam5Composite_Avg), x = 39.3, y = 1.4, size = 2.5)

grid.arrange(fam3s_plot, fam5s_plot, nrow=2)
```

These two graphs show the divide between the respondents at '3' and '5' familiarity. It seems the most familiar respondents have more impact on the disparity between the scores. As shown in the plots the differeces are driven by the increase in the most familiar respondents rather than decreases in the typically familiar respondents. The typically familiar respondents remains almost the same as the difference increases. 

The lurking variable is the extremeness in the scores among the more the familiar respondents. Specifically, it seems certain organizations are more able than others to breed extreme, but generally positive views among their higher familiar respondents.

The following is the distribution of PBI compared to the distribution of score differences between the high familiarity (5s and 4s) and typical familiarity (3s) respondents:

```{r message=FALSE, warning=FALSE}
###====High Familiarity Difference vs Composite Score Distribution 
cohL_Orgs_FamCut_MELT3 <- cohL_Orgs_FamCut %>%
  select(Company.Unique.Code.CYPHER.CL, Asso.CL, Main.Comparison.CL, 
         PercntDiff.5or4.and.3s, PercntDiff.Composite.and.AvgAllOrgs, Fam5or4Composite_Ns, Composite_Ns) %>%
  reshape(varying = list(c("PercntDiff.5or4.and.3s", "PercntDiff.Composite.and.AvgAllOrgs"), 
                         c("Fam5or4Composite_Ns","Composite_Ns")), 
          v.names=c("Composite.PercntDiff","Fam.Ns"),
          times = c("Difference b/w High Familiarity\nand Typical Familiarity",
                    "Distribution of Overall PBI scores"),
          sep = "_", direction = "long") %>%
  mutate(Key = ifelse(time == "Difference b/w High Familiarity\nand Typical Familiarity","Diff. b/w High Familiarity\nand Typical Familiarity",
                             ifelse(time == "Distribution of Overall PBI scores","Overall PBI",NA))) %>%
  mutate(Facet_Labels = ifelse(Asso.CL == 0, "Corporations", 
                               ifelse(Asso.CL == 1, "Associations", NA)))

formatter10 <- function(x){ 
  x/10 
}

#Density Plot: Visualizing High Familiarity Premium (or surplus)
#Difference between scores of high familiarity and typical familiarity
ggplot(data = cohL_Orgs_FamCut_MELT3, aes(x=Composite.PercntDiff, group=Key)) +
  geom_density(aes(fill = Key), colour = "grey", alpha = .5, size=0) +
  scale_x_continuous(limits=c(-.25, .3), labels = percent) + 
  scale_y_continuous(labels = formatter10) + 
  scale_fill_manual(values = c("slategrey","skyblue3")) +
  xlab("% Difference from the Average Org. PBI Score") + 
  ylab("Density Distribution") +
  ggtitle("Distribution of PBI and the Familarity Premium\n")
```

These two density plots show the way in which PBI scores and the familiarty premium are distributed. The familiarity premium measures the difference in scores given by high familiarity respondents and the typical familiarity respondents. 

The y-axis is the percentage difference from the average organization's PBI score. Zero in this chart represents the average score on PBI and each point is distance from the average organization's score. 

The typical organization tends to have a familiarity premium of about 17% above their PBI score. In other words, the average organization's high familiarity respondents evaluate it about 17% better than its actual score. 

The question is: what does it mean for an organization to have a high familiarity premium?

In literal terms, it means: policy influentials that know and engage with an organization regularily will develop a more positive view of that organization; while, policy influential that do not know the organization as well and have little engagement with it, do not have as sharp of an opinion of the organization.

Although, an organization that has a high familiarity premium equates to better scores among its more familiar respondents, this is not necessary "a good thing." Familiarity premium is similar to untapped potential. Potential is generally adventageous, however, an excess of untapped potential begs the question of "why it remains untapped?" 

In the same way, an organization that is reveared among policy makers that are highly familiar with it, but remains lukewarm among typically familiar policy makers reflects a missed oppertunity. 

For example, it could be that the particular organization enjoys a small group of "super fans," and no matter what this group of influentials have a quantifiablly higher perception of the organization. It could also be that while the organization just does a good job cultivating the relationships that they have, it fails at cultivating new relationships or lacks the expertise in translating their message boardly across the policy community.

The challenges for different organizations vary greatly. Highly visible, well established government affairs offices face different problems compared to newly created shops. Industry differences also require a very different set of engagement strategies. The banking industry faces far more hostility and prejudices than reletive newcomers in the policy space, such as the internet companies. There is not a "one-size fits all" answer to why a particular organization has a high or low familiarity premium. Hence, familiarity premium metric cannot perscribe a solution, rather it should be used as an warning indicator of a possible problem or oppertunity. It acts as a thermometer where having too high or too low of a suplus suggests that there could be an engagement or messaging issue.

###Conclusion

It's not that organizations with more familiar respondents do better, but rather respondents seem to develop more extreme views toward certain organizations the more familiar they are with them.

For the organizations that have a large pool of individuals that know them, increasing the number of individuals that know them will not lead to any more increases in their overall performance. They should rather look at the difference between high and low familiar respondents, the familiarity premium. If, for example, the gap between their high familiarity and typical familiarity respondents are significant, there might be a problem with messaging where their message is just not reaching the typical policy influencer. While, for the organizations that have very few individuals that know them, perhaps the root issue is visibility and conlcusion should be to increase their overall pool of very familiar influencials. 

###Program Specifications
```{r message=FALSE, warning=FALSE, echo=FALSE}
sessionInfo()
```

*Disclaimer: The views and opinions expressed in this article are those of the author's alone and do not necessarily reflect the official research findings or position of the National Journal Group. Examples of analysis performed within this article are only examples intended for achedemic purposes only. They are NOT to be utilized in real-world analytic products. If you are interested in learning more about this study or other products at National Journal Research please email <research@nationaljournal.com> or visit our website at [www.nationaljournal.com/membership/research](http://www.nationaljournal.com/learn-more/).*



~